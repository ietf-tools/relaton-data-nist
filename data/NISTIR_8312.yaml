---
schema-version: v1.2.9
id: NISTIR8312
title:
- content: Four principles of explainable artificial intelligence
  language:
  - en
  script:
  - Latn
  format: text/plain
  type: main
link:
- content: https://doi.org/10.6028/NIST.IR.8312
  type: doi
type: standard
docid:
- id: NIST IR 8312
  type: NIST
  primary: true
- id: 10.6028/NIST.IR.8312
  type: DOI
date:
- type: issued
  value: '2021'
contributor:
- person:
    name:
      completename:
        content: Phillips, P. Jonathon.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Hahn, Carina A.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Fontana, Peter C.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Yates, Amy N.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Greene, Kristen.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Broniatowski, David A.
        language:
        - en
  role:
  - type: author
- person:
    name:
      completename:
        content: Przybocki, Mark A.
        language:
        - en
  role:
  - type: author
- organization:
    name:
    - content: National Institute of Standards and Technology (U.S.)
    - content: Information Technology Laboratory
  role:
  - type: publisher
abstract:
- content: We introduce four principles for explainable artificial intelligence (AI)
    that comprise fundamental properties for explainable AI systems. We propose that
    explainable AI systems deliver accompanying evidence or reasons for outcomes and
    processes; provide explanations that are understandable to individual users; provide
    explanations that correctly reflect the system s process for generating the output;
    and that a system only operates under conditions for which it was designed and
    when it reaches sufficient confidence in its output. We have termed these four
    principles as explanation, meaningful, explanation accuracy, and knowledge limits,
    respectively. Through significant stakeholder engagement, these four principles
    were developed to encompass the multidisciplinary nature of explainable AI, including
    the fields of computer science, engineering, and psychology. Because one-sizefits-all
    explanations do not exist, different users will require different types of explanations.
    We present five categories of explanation and summarize theories of explainable
    AI. We give an overview of the algorithms in the field that cover the major classes
    of explainable algorithms. As a baseline comparison, we assess how well explanations
    provided by people follow our four principles. This assessment provides insights
    to the challenges of designing explainable AI systems.
  language:
  - en
  script:
  - Latn
  format: text/plain
series:
- title:
    content: |-
      NISTIR; NIST IR; NIST interagency report; NIST internal
                      report
    format: text/plain
  number: '8312'
place:
- city: Gaithersburg
  region:
  - name: Maryland
    iso: MD
doctype:
  type: standard
ext:
  schema-version: v1.0.0
